{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6ffe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/xphonebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from text2phonemesequence import Text2PhonemeSequence\n",
    "import torch\n",
    "\n",
    "# Load XPhoneBERT model and its tokenizer\n",
    "xphonebert = AutoModel.from_pretrained(\"vinai/xphonebert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/xphonebert-base\")\n",
    "\n",
    "# Load Text2PhonemeSequence\n",
    "# text2phone_model = Text2PhonemeSequence(language='eng-us', is_cuda=True)\n",
    "text2phone_model = Text2PhonemeSequence(language='jpn', is_cuda=True)\n",
    "\n",
    "# Input sequence that is already WORD-SEGMENTED (and text-normalized if applicable)\n",
    "sentence = \"That is , it is a testing text .\"  \n",
    "# sentence = \"これ は 、 テスト テキスト です .\"\n",
    "\n",
    "input_phonemes = text2phone_model.infer_sentence(sentence)\n",
    "\n",
    "input_ids = tokenizer(input_phonemes, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = xphonebert(**input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef11c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input phonemes: t a t ▁ i s ▁ , ▁ i t ɕ i ▁ i s ▁ a ▁ t ɛ s t i ŋ ▁ t e k s t ▁ .\n",
      "Input IDs: {'input_ids': tensor([[ 0,  7,  6,  7,  4,  8,  9,  4, 29,  4,  8,  7, 45,  8,  4,  8,  9,  4,\n",
      "          6,  4,  7, 17,  9,  7,  8, 35,  4,  7, 14, 13,  9,  7,  4, 33,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Features shape: torch.Size([1, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "features = features.last_hidden_state\n",
    "print(\"Input phonemes:\", input_phonemes)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Features shape:\", features.shape)\n",
    "# The features are the last hidden state of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dfa90",
   "metadata": {},
   "source": [
    "## Simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4c1ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11572 text files in ../../data/voicebank_demand/trainset_28spk_txt\n",
      "Processed 11572 text files\n",
      "Vocabulary built with 1000 words\n",
      "Tokenizer saved to simple_tokenizer.pkl\n",
      "\n",
      "Sample text: 'but you can go beyond that condition'\n",
      "Tokenized: [38, 34, 44, 117, 173, 14, 278]\n",
      "Decoded: but you can go beyond that condition\n",
      "\n",
      "Vocabulary size: 1000\n",
      "Top 10 words: ['<PAD>', '<UNK>', 'the', 'a', 'it', 'is', 'to', 'was', 'of', 'in', 'i', 'we']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        \"\"\"Simple word-level tokenizer with fixed vocabulary size.\"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.id_to_word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.next_id = 2  # Start from 2 (0=PAD, 1=UNK)\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Build vocabulary from texts based on word frequency.\"\"\"\n",
    "        # Count word occurrences\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Select top words for vocabulary\n",
    "        top_words = [word for word, _ in word_counts.most_common(self.vocab_size - 2)]  # -2 for <PAD> and <UNK>\n",
    "        \n",
    "        # Build mappings\n",
    "        for word in top_words:\n",
    "            if self.next_id < self.vocab_size:\n",
    "                self.word_to_id[word] = self.next_id\n",
    "                self.id_to_word[self.next_id] = word\n",
    "                self.next_id += 1\n",
    "        \n",
    "        print(f\"Vocabulary built with {len(self.word_to_id)} words\")\n",
    "        return self\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        return [self.word_to_id.get(word, self.word_to_id[\"<UNK>\"]) for word in words]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs back to words.\"\"\"\n",
    "        return [self.id_to_word.get(id, \"<UNK>\") for id in ids]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"Alias for decode to match HuggingFace API.\"\"\"\n",
    "        return self.decode(ids)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save tokenizer to file.\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'word_to_id': self.word_to_id,\n",
    "                'id_to_word': self.id_to_word,\n",
    "                'next_id': self.next_id\n",
    "            }, f)\n",
    "        print(f\"Tokenizer saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load tokenizer from file.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        tokenizer = cls(vocab_size=data['vocab_size'])\n",
    "        tokenizer.word_to_id = data['word_to_id']\n",
    "        tokenizer.id_to_word = data['id_to_word']\n",
    "        tokenizer.next_id = data['next_id']\n",
    "        \n",
    "        print(f\"Tokenizer loaded from {path} with {len(tokenizer.word_to_id)} words\")\n",
    "        return tokenizer\n",
    "\n",
    "def process_text_files(folder_path):\n",
    "    \"\"\"Process all .txt files in a folder and return cleaned texts.\"\"\"\n",
    "    punct_pattern = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "    texts = []\n",
    "    \n",
    "    # Get list of all text files\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    print(f\"Found {len(txt_files)} text files in {folder_path}\")\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in txt_files:\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                \n",
    "            # Remove punctuation and convert to lowercase\n",
    "            text = punct_pattern.sub(' ', text).lower()\n",
    "            # Normalize whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            texts.append(text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Processed {len(texts)} text files\")\n",
    "    return texts\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    folder_path = \"../../data/voicebank_demand/trainset_28spk_txt\"  # Change this to your folder path\n",
    "    vocab_size = 1000\n",
    "    output_path = \"simple_tokenizer.pkl\"\n",
    "    \n",
    "    # Process text files\n",
    "    texts = process_text_files(folder_path)\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer = SimpleTokenizer(vocab_size=vocab_size)\n",
    "    tokenizer.fit(texts)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save(output_path)\n",
    "    \n",
    "    # Example usage\n",
    "    if texts:\n",
    "        sample_text = texts[0][:100]  # First 100 chars of first text\n",
    "        print(f\"\\nSample text: '{sample_text}'\")\n",
    "        \n",
    "        tokens = tokenizer.tokenize(sample_text)\n",
    "        print(f\"Tokenized: {tokens}\")\n",
    "        \n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        print(f\"Decoded: {' '.join(decoded)}\")\n",
    "        \n",
    "        # Vocabulary stats\n",
    "        print(f\"\\nVocabulary size: {len(tokenizer.word_to_id)}\")\n",
    "        print(f\"Top 10 words: {list(tokenizer.word_to_id.keys())[:12]}\")  # First 12 includes <PAD> and <UNK>\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
