# configuration for Universe++ network with HiFi-GAN loss
_target_: open_universe.networks.universe_11May.UniverseGAN

fs: 16000

normalization_norm: 2
normalization_kwargs:
  ref: both
  level_db: -26.0

edm:
  noise: 0.25

score_model:
  _target_: open_universe.networks.universe_11May.ScoreNetwork
  fb_kernel_size: 3
  rate_factors: [2, 4, 4, 5]
  n_channels: 32
  n_rff: 32
  noise_cond_dim: 512
  encoder_gru_conv_sandwich: false
  extra_conv_block: true
  decoder_act_type: prelu
  use_weight_norm: true
  use_antialiasing: true
  time_embedding: simple

condition_model:
  _target_: open_universe.networks.universe_11May.ConditionerNetwork
  fb_kernel_size: ${model.score_model.fb_kernel_size}
  rate_factors: ${model.score_model.rate_factors}
  n_channels: ${model.score_model.n_channels}
  n_mels: 80
  n_mel_oversample: 4
  encoder_gru_residual: true
  extra_conv_block: ${model.score_model.extra_conv_block}
  decoder_act_type: prelu
  use_weight_norm: ${model.score_model.use_weight_norm}
  use_antialiasing: false
  text_encoder_config:
    _target_: open_universe.networks.universe_11May.TextEncoder ### NEW ###
    hidden_dim: 512 # 512 # 256
  text_lr_scale: 1.0 # UPD 04 MAY to 2.0 (= 0.5 reciprocal)
  mel_conditioning: "none" # "none"
  lat_conditioning: "full" # "full"
  
  # --- NEW: cross-attention hyper-params ------------------------
  cross_attention_num_heads: 8          # how many heads to use
  attention_temperature: 2.0 # UPD 04 MAY to 2.0 (= 0.5 reciprocal)


diffusion:
  schedule: geometric
  sigma_min: 0.0005
  sigma_max: 5.0
  n_steps: 8
  epsilon: 1.3

losses:
  multi_period_discriminator:
    mpd_reshapes: [2, 3, 5, 7, 11]
    use_spectral_norm: false
    discriminator_channel_mult: 1
  multi_resolution_discriminator:
    resolutions: [[1024, 120, 600], [2048, 240, 1200], [512, 50, 240]]
    use_spectral_norm: false
    discriminator_channel_mult: 1
  disc_freeze_step: 0
  weights:
    mel_l1: 45.0 #  60.0 # increased from 45.0 for stronger reconstruction
    score: 1.0
    # guided_attn: 10.0   # NEW 04 May: try 5â€“20; 0 disables GA loss
    guided_attn: 0.05   # 11 May - upd to 0.05 from 0.00
    # coverage: 0.1
    coverage: 0.1 ## 11 May - add it again
    ga_anneal_steps: 30000
  use_signal_decoupling: true
  signal_decoupling_act: snake
  score_loss:
    _target_: torch.nn.MSELoss

training:
  audio_len: ${datamodule.datasets.vb-train-16k.audio_len}
  time_sampling: time_normal_0.95
  dynamic_mixing: false
  ema_decay: 0.999

validation:
  main_loss: val/score # val/pesq # val/pesq default
  main_loss_mode: max
  n_bins: 5
  max_enh_batches: 4
  enh_losses:
    val/:
      _target_: open_universe.metrics.EvalMetrics
      audio_fs: ${model.fs}

optimizer:
  accumulate_grad_batches: 1
  generator:
    _target_: torch.optim.AdamW
    lr: 0.0002 # 0.00022 # 0.00035 # 0.0004  # increased generator learning rate from 0.0002 for 512 hidden_dim in condition model
    weight_decay: 0.01
    betas: [0.8, 0.99]
    weight_decay_exclude: [prelu, bias]
  discriminator:
    _target_: torch.optim.AdamW
    lr: 0.0002
    betas: [0.8, 0.99]
  grad_clip_vals:
    mrd: 1000.0
    mpd: 1000.0
    score: 1000.0
    cond: 1000.0

scheduler:
  generator:
    scheduler:
      _target_: open_universe.utils.schedulers.LinearWarmupCosineAnnealingLR
      T_warmup: 20000
      T_cosine: 400000
      eta_min: 1.6e-06
      T_max: ${trainer.max_steps}
    interval: step
    frequency: 1
  discriminator:
    scheduler:
      _target_: open_universe.utils.schedulers.LinearWarmupCosineAnnealingLR
      T_warmup: 20000
      T_cosine: 400000
      eta_min: 1.6e-06
      T_max: ${trainer.max_steps}
    interval: step
    frequency: 1
  text:                       # 5 k-step warm-up **only** on text ## NEW 30 APR
    scheduler:
      _target_: torch.optim.lr_scheduler.LinearLR
      start_factor: 1e-6  
      end_factor: 1.0
      total_iters: 5000
    interval: step
    frequency: 1

grad_clipper:
  _target_: open_universe.utils.FixedClipper
  max_norm: 1000.0
