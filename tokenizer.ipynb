{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e4e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11572 text files in ../../data/voicebank_demand/trainset_28spk_txt\n",
      "Processed 11572 text files\n",
      "Vocabulary built: 1038 tokens (words ≤ 1000, chars × 36)\n",
      "Tokenizer saved to simple_tokenizer.pkl\n",
      "\n",
      "Sample text: 'but you can go beyond that condition'\n",
      "Tokenized: [74, 70, 80, 153, 209, 50, 314]\n",
      "Decoded: but you can go beyond that condition\n",
      "\n",
      "Vocabulary size: 1038\n",
      "Top 10 words: ['<PAD>', '<UNK>', '<C_a>', '<C_b>', '<C_c>', '<C_d>', '<C_e>', '<C_f>', '<C_g>', '<C_h>', '<C_i>', '<C_j>']\n"
     ]
    }
   ],
   "source": [
    "# simple_tokenizer.py\n",
    "import os, re, string, pickle\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    Word-level tokenizer with:\n",
    "      • special tokens  : <PAD> =0, <UNK>=1\n",
    "      • optional fixed-size *word* vocab\n",
    "      • guaranteed 1-char tokens <C_a> … <C_z>, <C_0> … <C_9>\n",
    "        so an OOV word is split into characters instead of <UNK>.\n",
    "    \"\"\"\n",
    "\n",
    "    CHAR_TOKENS = list(string.ascii_lowercase) + list(string.digits)\n",
    "\n",
    "    def __init__(self, vocab_size: int | None = 10_000, add_char_fallback: bool = True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int | None\n",
    "            Max *word* tokens *excluding* special + char tokens.\n",
    "            None ⇒ unlimited.\n",
    "        add_char_fallback : bool\n",
    "            If True, reserves tokens <C_a> … and uses them when a word is OOV.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.add_char_fallback = add_char_fallback\n",
    "\n",
    "        self.word_to_id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.id_to_word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.next_id = 2  # start after PAD, UNK\n",
    "\n",
    "        # will be filled in fit()\n",
    "        self.char_token_ids: dict[str, int] = {}\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────\n",
    "    # public API\n",
    "    # ────────────────────────────────────────────────────────────────────────\n",
    "    def fit(self, texts: List[str]):\n",
    "        \"\"\"Build vocabulary from a list of raw texts.\"\"\"\n",
    "        # 1) optional char tokens\n",
    "        if self.add_char_fallback:\n",
    "            for ch in self.CHAR_TOKENS:\n",
    "                self._add_token(f\"<C_{ch}>\", force=True)   # always add\n",
    "\n",
    "        # 2) word statistics\n",
    "        counts = Counter()\n",
    "        for txt in texts:\n",
    "            counts.update(txt.lower().split())\n",
    "\n",
    "        # 3) take most common words up to vocab_size\n",
    "        limit = (self.vocab_size or len(counts))           # None → unlimited\n",
    "        for word, _ in counts.most_common(limit):\n",
    "            self._add_token(word)\n",
    "\n",
    "        # store quick look-up for char fallback\n",
    "        self.char_token_ids = {\n",
    "            ch: self.word_to_id.get(f\"<C_{ch}>\") for ch in self.CHAR_TOKENS\n",
    "        }\n",
    "\n",
    "        print(f\"Vocabulary built: {len(self.word_to_id)} tokens \"\n",
    "              f\"(words ≤ {self.vocab_size}, chars × {len(self.CHAR_TOKENS)})\")\n",
    "        return self\n",
    "\n",
    "    def tokenize(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert a sentence to a list of token IDs with char fallback.\"\"\"\n",
    "        ids: List[int] = []\n",
    "        for word in text.lower().split():\n",
    "            tid = self.word_to_id.get(word)\n",
    "            if tid is not None:\n",
    "                ids.append(tid)\n",
    "            elif self.add_char_fallback:\n",
    "                # decompose into characters\n",
    "                ids.extend(self.char_token_ids.get(ch, 1)  # 1 = <UNK> char\n",
    "                           for ch in word)\n",
    "            else:\n",
    "                ids.append(1)      # <UNK>\n",
    "        return ids or [1]           # never return []\n",
    "\n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        \"\"\"IDs → tokens (words or <C_x>).\"\"\"\n",
    "        return [self.id_to_word.get(i, \"<UNK>\") for i in ids]\n",
    "\n",
    "    # alias for HuggingFace-style API\n",
    "    convert_ids_to_tokens = decode\n",
    "\n",
    "    # ───────────────────────────────────────────── storage helpers ──────────\n",
    "    def save(self, path):\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "        print(f\"Tokenizer saved to {path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        tok = cls(vocab_size=state[\"vocab_size\"],\n",
    "                  add_char_fallback=state[\"add_char_fallback\"])\n",
    "        tok.__dict__.update(state)\n",
    "        print(f\"Tokenizer loaded from {path} ({len(tok.word_to_id)} tokens)\")\n",
    "        return tok\n",
    "\n",
    "    # ───────────────────────────────────────────── private  ────────────────\n",
    "    def _add_token(self, token: str, force: bool = False):\n",
    "        \"\"\"Add a single token to vocab (internal).\"\"\"\n",
    "        if token in self.word_to_id:\n",
    "            return\n",
    "        if (self.vocab_size is not None\n",
    "                and not force\n",
    "                and (self.next_id - 2 - len(self.CHAR_TOKENS)) >= self.vocab_size):\n",
    "            return      # reached word budget\n",
    "        self.word_to_id[token] = self.next_id\n",
    "        self.id_to_word[self.next_id] = token\n",
    "        self.next_id += 1\n",
    "\n",
    "\n",
    "def process_text_files(folder_path):\n",
    "    \"\"\"Process all .txt files in a folder and return cleaned texts.\"\"\"\n",
    "    punct_pattern = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "    texts = []\n",
    "    \n",
    "    # Get list of all text files\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    print(f\"Found {len(txt_files)} text files in {folder_path}\")\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in txt_files:\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                \n",
    "            # Remove punctuation and convert to lowercase\n",
    "            text = punct_pattern.sub(' ', text).lower()\n",
    "            # Normalize whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            texts.append(text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Processed {len(texts)} text files\")\n",
    "    return texts\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    folder_path = \"../../data/voicebank_demand/trainset_28spk_txt\"  # Change this to your folder path\n",
    "    vocab_size = 1000\n",
    "    output_path = \"simple_tokenizer.pkl\"\n",
    "    \n",
    "    # Process text files\n",
    "    texts = process_text_files(folder_path)\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer = SimpleTokenizer(vocab_size=vocab_size)\n",
    "    tokenizer.fit(texts)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save(output_path)\n",
    "    \n",
    "    # Example usage\n",
    "    if texts:\n",
    "        sample_text = texts[0][:100]  # First 100 chars of first text\n",
    "        print(f\"\\nSample text: '{sample_text}'\")\n",
    "        \n",
    "        tokens = tokenizer.tokenize(sample_text)\n",
    "        print(f\"Tokenized: {tokens}\")\n",
    "        \n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        print(f\"Decoded: {' '.join(decoded)}\")\n",
    "        \n",
    "        # Vocabulary stats\n",
    "        print(f\"\\nVocabulary size: {len(tokenizer.word_to_id)}\")\n",
    "        print(f\"Top 10 words: {list(tokenizer.word_to_id.keys())[:12]}\")  # First 12 includes <PAD> and <UNK>\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
